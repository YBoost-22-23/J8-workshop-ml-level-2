{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YBoost-22-23/J8-workshop-ml-level-2/blob/main/Workshop_Yboost_Exercice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "source": [
        "!apt-get update && apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines3[extra] pyglet==1.5.27"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install swig cmake"
      ],
      "metadata": {
        "id": "ZMAfMF_wu2HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install box2d-py\n"
      ],
      "metadata": {
        "id": "6I9uqDAjvSgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DLR-RM/stable-baselines3.git\n",
        "%cd stable-baselines3\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "I45ETbRw-GZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U29X1-B-AIKE"
      },
      "source": [
        "import stable_baselines3\n",
        "stable_baselines3.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Prepare video recording"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make('LunarLander-v2')])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcX8hEcaUpR0"
      },
      "source": [
        "Stable-Baselines works on environments that follow the [gym interface](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html).\n",
        "You can find a list of available environment [here](https://gym.openai.com/envs/#classic_control).\n",
        "\n",
        "It is also recommended to check the [source code](https://github.com/openai/gym) to learn more about the observation and action space of each env, as gym does not have a proper documentation.\n",
        "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines.readthedocs.io/en/master/guide/algos.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import MlpPolicy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understand what is Gym and how it works ü§ñ\n",
        "\n",
        "üèã The library containing our environment is called Gym.\n",
        "**You'll use Gym a lot in Deep Reinforcement Learning.**\n",
        "\n",
        "The Gym library provides two things:\n",
        "- An interface that allows you to **create RL environments**.\n",
        "- A **collection of environments** (gym-control, atari, box2D...).\n",
        "\n",
        "Let's look at an example, but first let's remember what's the RL Loop.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"The RL process\" width=\"100%\">"
      ],
      "metadata": {
        "id": "_Vx1S5uDI5i6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At each step:\n",
        "- Our Agent receives¬†**state S0**¬†from the¬†**Environment**¬†‚Äî we receive the first frame of our game (Environment).\n",
        "- Based on that¬†**state S0,**¬†the Agent takes¬†**action A0**¬†‚Äî our Agent will move to the right.\n",
        "- Environment to a¬†**new**¬†**state S1**¬†‚Äî new frame.\n",
        "- The environment gives some¬†**reward R1**¬†to the Agent ‚Äî we‚Äôre not dead¬†*(Positive Reward +1)*.\n",
        "\n",
        "\n",
        "With Gym:\n",
        "\n",
        "1Ô∏è‚É£ We create our environment using `gym.make()`\n",
        "\n",
        "2Ô∏è‚É£ We reset the environment to its initial state with `observation = env.reset()`\n",
        "\n",
        "At each step:\n",
        "\n",
        "3Ô∏è‚É£ Get an action using our model (in our example we take a random action)\n",
        "\n",
        "4Ô∏è‚É£ Using `env.step(action)`, we perform this action in the environment and get\n",
        "- `observation`: The new state (st+1)\n",
        "- `reward`: The reward we get after executing the action\n",
        "- `done`: Indicates if the episode terminated\n",
        "- `info`: A dictionary that provides additional information (depends on the environment).\n",
        "\n",
        "If the episode is done:\n",
        "- We reset the environment to its initial state with `observation = env.reset()`\n",
        "\n",
        "**Let's look at an example!** Make sure to read the code\n"
      ],
      "metadata": {
        "id": "25EC1R0dJDz9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "source": [
        "# First, we create our environment called LunarLander-v2\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# Then we reset this environment\n",
        "observation = env.reset()\n",
        "\n",
        "for _ in range(20):\n",
        "  # Take a random action\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action taken:\", action)\n",
        "\n",
        "  # Do this action in the environment and get\n",
        "  # next_state, reward, done and info\n",
        "  observation, reward, done, info = env.step(action)\n",
        "  \n",
        "  # If the game is done (in our case we land, crashed or timeout)\n",
        "  if done:\n",
        "      # Reset the environment\n",
        "      print(\"Environment is reset\")\n",
        "      observation = env.reset()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above you can see printed which movement decision our lunarlander took each step for 20 steps ! For now its pretty random."
      ],
      "metadata": {
        "id": "8gFFD_MDJSgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see what the Environment looks like:\n"
      ],
      "metadata": {
        "id": "xNOHof4BJ3NR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "env.reset()\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", env.observation_space.shape)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "id": "gZq006OqGZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, where each value contains different information about the lander:\n",
        "- Horizontal pad coordinate (x)\n",
        "- Vertical pad coordinate (y)\n",
        "- Horizontal speed (x)\n",
        "- Vertical speed (y)\n",
        "- Angle\n",
        "- Angular speed\n",
        "- If the left leg has contact point touched the land\n",
        "- If the right leg has contact point touched the land\n"
      ],
      "metadata": {
        "id": "iMPHP1BnGhpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "id": "Uqk1ZOvAHNCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ: \n",
        "\n",
        "- Do nothing,\n",
        "- Fire left orientation engine,\n",
        "- Fire the main engine,\n",
        "- Fire right orientation engine.\n",
        "\n",
        "Reward function (the function that will gives a reward at each timestep) üí∞:\n",
        "\n",
        "- Moving from the top of the screen to the landing pad and zero speed is about 100~140 points.\n",
        "- Firing main engine is -0.3 each frame\n",
        "- Each leg ground contact is +10 points\n",
        "- Episode finishes if the lander crashes (additional - 100 points) or come to rest (+100 points)"
      ],
      "metadata": {
        "id": "ngtT-zrxHVRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Model ü§ñ\n",
        "- Now that we studied our environment and we understood the problem: **being able to land correctly the Lunar Lander to the Landing Pad by controlling left, right and main orientation engine**. Let's build the algorithm we're going to use to solve this Problem üöÄ.\n",
        "\n",
        "- To do so, we're going to use our first Deep RL library, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).\n",
        "\n",
        "- SB3 is a set of **reliable implementations of reinforcement learning algorithms in PyTorch**.\n",
        "\n",
        "---\n",
        "\n",
        "üí° A good habit when using a new library is to dive first on the documentation: https://stable-baselines3.readthedocs.io/en/master/ and then try some tutorials.\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "2zNui5StLQql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create a new environment for the creation of our model"
      ],
      "metadata": {
        "id": "Q6JltU_Qrfjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve this problem, we're going to use SB3 **PPO**. [PPO (aka Proximal Policy Optimization) is one of the of the SOTA (state of the art) Deep Reinforcement Learning algorithms](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).\n",
        "\n",
        "PPO is a combination of:\n",
        "- *Value-based reinforcement learning method*: learning an action-value function that will tell us what's the **most valuable action to take given a state and action**.\n",
        "- *Policy-based reinforcement learning method*: learning a policy that will **gives us a probability distribution over actions**.\n"
      ],
      "metadata": {
        "id": "xFT1DEuoLzXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stable-Baselines3 is easy to set up:\n",
        "\n",
        "1Ô∏è‚É£ You **create your environment** (in our case it was done above)\n",
        "\n",
        "2Ô∏è‚É£ You define the **model you want to use and instantiate this model** `model = PPO(\"MlpPolicy\")`\n",
        "\n",
        "3Ô∏è‚É£ You **train the agent** with `model.learn` and define the number of training timesteps\n",
        "\n",
        "```\n",
        "# Create environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "# Instantiate the agent\n",
        "model = PPO('MlpPolicy', env, verbose=1)\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=int(2e5))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "C2AnVxWEMImv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO(\n",
        "    policy = 'MlpPolicy',\n",
        "    env = env,\n",
        "    n_steps = 1024,\n",
        "    batch_size = 64,\n",
        "    n_epochs = 4,\n",
        "    gamma = 0.999,\n",
        "    gae_lambda = 0.98,\n",
        "    ent_coef = 0.01,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "Ly4XVltkMlB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the PPO agent üèÉ\n",
        "- Let's train our agent for 100 timesteps, don't forget to use GPU on Colab."
      ],
      "metadata": {
        "id": "s36F6HBkM9zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train it for 100 timesteps\n",
        "\n",
        "# Specify file name for model and save the model to file\n",
        "model_name = \"ppo-LunarLander-v2\"\n",
        "model.save(model_name)"
      ],
      "metadata": {
        "id": "uUBXbjhdNT1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the agent üìà \n",
        "- Now that our Lunar Lander agent is trained üöÄ, we need to **check its performance**.\n",
        "- Stable-Baselines3 provides a method to do that: `evaluate_policy`.\n",
        "- To fill that part you need to [check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)\n",
        "\n",
        "\n",
        "üí° When you evaluate your agent, you should not use your training environment but create an evaluation environment."
      ],
      "metadata": {
        "id": "fy-oCSocNn56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "\n",
        "env_id = \"LunarLander-v2\"\n",
        "\n",
        "# Create a new environment for evaluation\n",
        "eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "\n",
        "# TODO: Evaluate the model with parameters: 10 evaluation episodes and deterministic=True \n",
        "mean_reward, std_reward = #?methodName?#(model, eval_env, #?specifyNbrEpisodes?#, #?Enableeterministic?#)\n",
        "\n",
        "# Print the results\n",
        "print(f\"mean_reward= {mean_reward:.2f} +/- {std_reward}\")\n",
        "print(f\"score= {mean_reward / std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "jeKtXVJYNxAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOObbeu5MMlR"
      },
      "source": [
        "### Visualize trained agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iATu7AiyMQW2"
      },
      "source": [
        "record_video('LunarLander-v2', model, video_length=500, prefix='ppo-lunar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n4i-fW3NojZ"
      },
      "source": [
        "show_videos('videos', prefix='ppo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some additional challenges üèÜ\n",
        "The best way to learn **is to try things by your own**! As you saw, the current agent is not doing great. So now manage on your own to improve your little agent and let's create a leaderboard in the class !\n",
        "\n",
        "\n",
        "Here are some ideas to achieve so:\n",
        "* Train more steps\n",
        "* Try different hyperparameters of `PPO`. You can see them at https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters. "
      ],
      "metadata": {
        "id": "FgC6OzkSPNrX"
      }
    }
  ]
}